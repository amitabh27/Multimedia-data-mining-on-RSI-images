import numpy as np
import pandas as pd
import re, nltk, gensim

from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import GridSearchCV
from pprint import pprint

import pyLDAvis
import pyLDAvis.sklearn
import matplotlib.pyplot as plt
%matplotlib inline

from  urllib.request import urlopen
from bs4 import BeautifulSoup


#It will contain all the corpus at end as a list of articles
data=[]
#It contains urls of all web pages from where articles from morgan stanley site are picked up
article_url=[]
base_url='www.morganstanley.com'


url='http://www.morganstanley.com/ideas'
page = urlopen(url)
soup = BeautifulSoup(page,"html5lib")
needed_div=soup.find('section',{"id":"tiles"})

for link in needed_div.findAll('a', attrs={'href': re.compile("/")}):
    article_url.append(link.get('href'))



url='http://www.morganstanley.com/what-we-do/wealth-management/family.html'
page = urlopen(url)
soup = BeautifulSoup(page,"html5lib")
needed_div=soup.find('section',{"id":"tiles"})

for link in needed_div.findAll('a', attrs={'href': re.compile("/")}):
    if "https" not in link.get('href'):
        article_url.append(link.get('href'))
        


stopping_len=len(article_url)

url='http://www.morganstanley.com/spc/knowledge/managing-wealth/wealth-management-for-stock-plan-participants/'
page = urlopen(url)
soup = BeautifulSoup(page,"html5lib")

for i in soup.findAll('ul',{"class":"tertiary leftNav reversePageOrder"}):
    for link in i.findAll('a', attrs={'href': re.compile("/")}):
        if "https" not in link.get('href'):
            article_url.append(link.get('href'))
            




j = 0
for i in article_url:

    page_url='http://'+base_url+i
    
    print(page_url)
    
    page = urlopen(page_url)

    soup = BeautifulSoup(page,"html5lib")
    #print(soup.prettify())
    article_content=' '
    
    
    j += 1
    if j <= stopping_len : 
    
        for curr_div in soup.find_all('div',{"class":"bodytext section"}):
            inner_div=curr_div.find('div')
            innertext=inner_div.decode_contents()
    
            innertext=innertext.replace("&amp;", "&")
            innertext=innertext.replace("\n", "")
            innertext = re.sub('<.*?>', '', innertext)
            innertext = re.sub('</.*?>', '', innertext)
            article_content=article_content+innertext
    else:
        
        for curr_div in soup.find_all('div',{"class":"text parbase section"}):
            innertext=curr_div.decode_contents()
    
            innertext=innertext.replace("&amp;", "&")
            innertext=innertext.replace("\n", "")
            innertext = re.sub('<.*?>', '', innertext)
            innertext = re.sub('</.*?>', '', innertext)
            article_content=article_content+innertext
        
    
    #print(article_contnet)
    data.append(article_content)  


*****


#data
len(data)

****
#print(data[:1])

****
#word_tokenizing

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))
        
data_words = list(sent_to_words(data))

#print(data_words[:1])
*******


#stop words removal : removing stop words of language=english
#stemming : getting the root words

from nltk.stem.porter import PorterStemmer
from stop_words import get_stop_words

p_stemmer = PorterStemmer()
en_stop = get_stop_words('en')

data_lemmatized = []

for i in data_words:
    tokens = i
    stopped_tokens = [i for i in tokens if not i in en_stop]
    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]
    
    data_lemmatized.append(' '.join(stemmed_tokens))
    
#print(data_lemmatized[:1])
    
    
    *******
    
    
    
    vectorizer = CountVectorizer(analyzer='word',       
                             #min_df=10,                        # minimum reqd occurences of a word 
                             stop_words='english',             # remove stop words
                             lowercase=True,                   # convert all words to lowercase
                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3
                             # max_features=30000,             # max number of uniq words
                            )

data_vectorized = vectorizer.fit_transform(data_lemmatized)

****

#analyzing sparcity : percentage of non-zero data-points in document-word matrix
data_dense = data_vectorized.todense()
print("Sparsicity: ", ((data_dense > 0).sum()/data_dense.size)*100, "%")

******

#Building LDA model
lda_model = LatentDirichletAllocation(n_components=10,               # Number of topics
                                      max_iter=20,               # Max learning iterations
                                      learning_method='online',   
                                      random_state=100,          # Random state
                                      batch_size=2,            # n docs in each learning iter
                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't
                                      n_jobs = -1,               # Use all available CPUs
                                     )
lda_output = lda_model.fit_transform(data_vectorized)

print(lda_model)  # Model attributes

******

#analyzing diagnosis parameters
print("Log Likelihood: ", lda_model.score(data_vectorized))
print("Perplexity: ", lda_model.perplexity(data_vectorized))

pprint(lda_model.get_params())

*****

#Applying Grid Search to find best LDA model


# Define Search Param
search_params = {'n_components': [10,15,20,25], 'learning_decay': [.5, .7, .9]}

# Init the Model
lda = LatentDirichletAllocation()

# Init Grid Search Class
model = GridSearchCV(lda, param_grid=search_params)

# Do the Grid Search
model.fit(data_vectorized)

*****


# Printing params for best model among all the generated ones
# Best Model
best_lda_model = model.best_estimator_

# Model Parameters
print("Best Model's Params: ", model.best_params_)

# Log Likelihood Score
print("Best Log Likelihood Score: ", model.best_score_)

# Perplexity
print("Model Perplexity: ", best_lda_model.perplexity(data_vectorized))

*****

#compare performance scores of models using matploitlib
# Get Log Likelyhoods from Grid Search Output
n_topics = [10,15,20,25]
log_likelyhoods_5 = [round(gscore.mean_validation_score) for gscore in model.grid_scores_ if gscore.parameters['learning_decay']==0.5]
log_likelyhoods_7 = [round(gscore.mean_validation_score) for gscore in model.grid_scores_ if gscore.parameters['learning_decay']==0.7]
log_likelyhoods_9 = [round(gscore.mean_validation_score) for gscore in model.grid_scores_ if gscore.parameters['learning_decay']==0.9]

# Show graph
plt.figure(figsize=(12, 8))
plt.plot(n_topics, log_likelyhoods_5, label='0.5')
plt.plot(n_topics, log_likelyhoods_7, label='0.7')
plt.plot(n_topics, log_likelyhoods_9, label='0.9')
plt.title("Choosing Optimal LDA Model")
plt.xlabel("Num Topics")
plt.ylabel("Log Likelyhood Scores")
plt.legend(title='Learning decay', loc='best')
plt.show()

******

#dominant topic in each doc

# Create Document - Topic Matrix
lda_output = best_lda_model.transform(data_vectorized)

# column names
topicnames = ["Topic" + str(i) for i in range(best_lda_model.n_components)]

# index names
docnames = ["Doc" + str(i) for i in range(len(data))]

# Make the pandas dataframe
df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)

# Get dominant topic for each document
dominant_topic = np.argmax(df_document_topic.values, axis=1)
df_document_topic['dominant_topic'] = dominant_topic

# Styling
def color_green(val):
    color = 'green' if val > .1 else 'black'
    return 'color: {col}'.format(col=color)

def make_bold(val):
    weight = 700 if val > .1 else 400
    return 'font-weight: {weight}'.format(weight=weight)

# Apply Style
df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)
df_document_topics

******

df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name="Num Documents")
df_topic_distribution.columns = ['Topic Num', 'Num Documents']
df_topic_distribution


******

pyLDAvis.enable_notebook()
panel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer,mds='tsne')
panel

******

# defining topic keywords 
# Topic-Keyword Matrix
df_topic_keywords = pd.DataFrame(best_lda_model.components_)

# Assign Column and Index
df_topic_keywords.columns = vectorizer.get_feature_names()
df_topic_keywords.index = topicnames

# View
df_topic_keywords

******

#get top 15 keywords for each doc


# Show top n keywords for each topic
def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):
    keywords = np.array(vectorizer.get_feature_names())
    topic_keywords = []
    for topic_weights in lda_model.components_:
        top_keyword_locs = (-topic_weights).argsort()[:n_words]
        topic_keywords.append(keywords.take(top_keyword_locs))
    return topic_keywords

topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=15)        

# Topic - Keywords Dataframe
df_topic_keywords = pd.DataFrame(topic_keywords)
df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]
df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]
df_topic_keywords

*****
#Given a piece of text, predicting the topic in document

def predict_topic(text):
    global sent_to_words
    
    mytext_2 = list(sent_to_words(text))
    #print(mytext_2)
    
    mytext_3 =[]
     
    for i in mytext_2 :
        
        tokens=i
        stopped_tokens = [i for i in tokens if not i in en_stop]
        #print(stopped_tokens)
        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]
        #print(stemmed_tokens)
        mytext_3.append(' '.join(stemmed_tokens))
        #print(mytext_3)
    
        mytext_4 = vectorizer.transform(mytext_3)
    
    topic_probability_scores = best_lda_model.transform(mytext_4)
    topic = df_topic_keywords.iloc[np.argmax(topic_probability_scores), :].values.tolist()
    return topic, topic_probability_scores


mytext = ["are you saving much for your children. Be responsible and start saving for your children from young age so that the family gets a good financial cover"]
topic, prob_scores = predict_topic(text = mytext)
print(topic)
    
    
    
    
    *******
    
    
    #Given a piece of Text, predicting the documents that are related to it most closely

from sklearn.metrics.pairwise import euclidean_distances

def similar_documents(text, doc_topic_probs, documents = data, top_n=5, verbose=False):
    topic, x  = predict_topic(text)
    dists = euclidean_distances(x.reshape(1, -1), doc_topic_probs)[0]
    doc_ids = np.argsort(dists)[:top_n]
    if verbose:        
        print("Topic KeyWords: ", topic)
        print("Topic Prob Scores of text: ", np.round(x, 1))
        print("Most Similar Doc's Probs:  ", np.round(doc_topic_probs[doc_ids], 1))
    return doc_ids, np.take(documents, doc_ids)

# Get similar documents
mytext = ["Some text about old age financial cover by start saving when you are young"]
doc_ids, docs = similar_documents(text=mytext, doc_topic_probs=lda_output, documents = data, top_n=2, verbose=True)
print('\n', docs[0][:500])
print('\n\n\n\n', docs[1][:500])

