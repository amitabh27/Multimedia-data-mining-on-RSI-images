
Hard Clustering = Each feature vector belongs to any one cluster.
Soft clustering = A feature vector belongs to all possible clusters made such that the sum of all probabilities of it belonging to cluster Ci is 1.

Hard = Kmeans
http://www.business-science.io/business/2016/08/07/CustomerSegmentationPt1.html ---> Github dataset
https://www.youtube.com/watch?v=ikt0sny_ImY ---> Youtube Scikit Kmeans implementation 

Pre-requisities:
scale your variables
Look at the scatterplot or data table to estimate number of clusters.

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Customer Segmentation using Kmeans:
http://www.business-science.io/business/2016/08/07/CustomerSegmentationPt1.html
http://www.kimberlycoffey.com/blog/2016/8/k-means-clustering-for-customer-segmentation
https://subhayo.wordpress.com/2017/08/15/customer-segmentation-using-k-means-clustering/
http://inseaddataanalytics.github.io/INSEADAnalytics/CourseSessions/Sessions45/ClusterAnalysisReading.html
https://www.analyticsvidhya.com/blog/2013/11/getting-clustering-right/

https://stats.stackexchange.com/questions/28170/clustering-a-dataset-with-both-discrete-and-continuous-variables ---> Discrete vs Continuous attributes in Kmeans? How to approach

http://www.ritchieng.com/machine-learning-project-customer-segments/ ---> GMM and Kmeans implementation sklearn python 



----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

https://pypi.org/project/kmodes/ ---> Kmodes algorithm which is built on top of Kmeans to handle both categorical and discrete data
https://datascience.stackexchange.com/questions/8681/clustering-for-mixed-numeric-and-nominal-discrete-data -- > R has Gower Distance to handle both categorical and discrete data
Also Check K-prototypes ---> which applies to data with a mix of categorical and numeric features. It uses a distance measure which mixes the Hamming distance for categorical features and the Euclidean distance for numeric features.


One-hot representation to deal with both categorical and continuous attribute:
have some categorical variable called "color" that could take on the values red, blue, or yellow. If we simply encode these numerically as 1,2, and 3 respectively, 
our algorithm will think that red (1) is actually closer to blue (2) than it is to yellow (3). We need to use a representation that lets the computer understand that these things are all actually equally different.
One simple way is to use what's called a one-hot representation, and it's exactly what you thought you should do. Rather than having one variable like "color" that can take on three values, 
we separate it into three variables. These would be "color-red," "color-blue," and "color-yellow," which all can only take on the value 1 or 0.

It depends on your categorical variable being used. For ordinal variables, say like bad,average and good, it makes sense just to use one variable and have values 0,1,2 and distances make sense here
(Avarage is closer to bad and good).
However, if there is no order, you should ideally use one hot encoding as mentioned above.



Categorical--->Kmodes
Numerical---->Kmeans
https://medium.com/@Chamanijks/k-prototype-in-clustering-mixed-attributes-e6907db91914 ---- > If your data is both categorical and numerical use Kprototype a combination of Kmodes and Kmeans.

























Soft = Fuzzy c-Means(FCM), Kmeans through GMM.


Fuzzy c-Means :
---------------------------------------------------------------------------
https://en.wikipedia.org/wiki/Fuzzy_clustering#Fuzzy_C-means_clustering













#####################################################################################################################################################################################################################################


LDA :

Concept = https://www.youtube.com/watch?v=DWJYZq_fQ2A

Implementation PYTHON = https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/
BEST CODE = https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html
LDA Classification after results come in above links = https://blog.algorithmia.com/lda-algorithm-classify-text-documents/

Guided LDA = https://medium.freecodecamp.org/how-we-changed-unsupervised-lda-to-semi-supervised-guidedlda-e36a95f3a164 = 
https://github.com/vi3k6i5/GuidedLDA
https://guidedlda.readthedocs.io/en/latest/
Here you can pre-specify the seed words that form a topic so that the Unsupervised LDA becomes Supervised
LDA.

AWESOME LDA = https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/
https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/



########################################################################################################################################




1. Vector of Words Model = Good for clustering documents that have high resemblance. It works on resemblance by considering document as a whole.

2. Named Entity Recognition= Good for fine-grain level search in a document like clustering documents that have phrase "MS". It works on resemblance by considering specific phrases.
	(stanford-ner.jar needed)
	

3.Latent Dirichelet Allocation (LDA) or sLDA :


4. Correlated Topic Models :

5. kNN :
The tags to be assigned to a document depends on its similar % to k-Nearest Neighbours. Initially documents are given labels as initial corpus.

6.automated tools for tagging :
KEA(keyphrase extraction algo) and Maui a tool by google

	
	
Books:

*Parallel Dots API for identifying features.
*Lexalytics API



*https://stanfordnlp.github.io/CoreNLP/
https://nlp.stanford.edu/software/crf-faq.shtml#a --> Training model using standford NER
https://www.youtube.com/watch?v=w4rWpvBjBRI ---> Java implementation
https://www.nltk.org/book/ch07.html --->  Flow diagram of NLP nicely explained.

https://github.com/dat/stanford-ner --> as servlet mode 
http://www.davidsbatista.net/blog/2018/01/23/StanfordNER/ --> as servlet mode


https://robots.thoughtbot.com/named-entity-recognition
https://pythonprogramming.net/using-bio-tags-create-named-entity-lists/?completed=/testing-stanford-ner-taggers-for-speed/
https://stackoverflow.com/questions/30664677/extract-list-of-persons-and-organizations-using-stanford-ner-tagger-in-nltk
https://www.youtube.com/watch?v=FLZvOKSCkxY&list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL ---> Youtube NLTK python tutorials
http://mattshomepage.com/articles/2016/May/23/nltk_nec/ ---> Python NER
https://www.commonlounge.com/discussion/2662a77ddcde4102a16d5eb6fa2eff1e
http://www.informit.com/articles/article.aspx?p=2265404
https://ganeshpachpind.wordpress.com/2014/02/28/stanford-named-entity-recognizer/
https://dataturks.com/blog/stanford-core-nlp-ner-training-java-example.php?s=so



http://www.stewh.com/2013/11/extracting-named-entities-in-c-using-the-stanford-nlp-parser/ -----> C# Stanford NLP example


https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/
	
